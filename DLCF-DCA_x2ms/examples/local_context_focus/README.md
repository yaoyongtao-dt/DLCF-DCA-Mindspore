# Local Context Focus mechanism

**Local Context Focus (LCF)** mechanism is a simple and adoptive method proposed for ABSA. Most previous works divided
the input sequence into aspect term sequence and context sequence, then models their interrelation. We find that apart
from the global context, the local context of aspect terms contains more significant information. Therefore, we propose
the **Semantic-Relative Distance (SRD)** to measure the relevance between the aspect term and its context words, aiming
to assist the model to capture local context. In addition, we design a **Local Context Focus (LCF)** mechanism to pay
more attention to the local context words whose SRD does not exceed the **predefined fixed threshold**. According to our
experiments in SemEval-2014, 15, and 16, LCF works best when the **predefined fixed threshold is** set **3**. LCF
contains two patterns: **Context Dynamic Weighted (CDW)** and **Context Dynamic Masked (CDM)**. More detail about LCF is
available in [our paper](https://www.mdpi.com/2076-3417/9/16/3389).

## LCF architecture

![lcf](pic/lcf.png)

## The simulation of Context Dynamic Weighted (CDW)

CDW weight down the less-semantic-relative context words.
![cdw](pic/cdw.png)

## The simulation of Context Dynamic Masked (CDM)

CDW masked the less-semantic-relative context words.
![cdm](pic/cdm.png)

# Some works related to LCF

[1] Phan M H, Ogunbona P O. Modelling context and syntactical features for aspect-based sentiment analysis[C]
//Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020: 3211-3220.

[2] Hyun D, Cho J, Yu H. Building Large-Scale English and Korean Datasets for Aspect-Level Sentiment Analysis in
Automotive Domain[C]//Proceedings of the 28th International Conference on Computational Linguistics. 2020: 961-966.

[3] Yang H, Zeng B, Yang J H, et al. A multi-task learning model for chinese-oriented aspect polarity classification and
aspect term extraction[J]. Neurocomputing, 2021, 419: 344-356.

[4] Ke W, Gao J, Shen H, et al. Incorporating Explicit Syntactic Dependency for Aspect Level Sentiment Classification[J]
. Neurocomputing, 2021.

[5] Luo W, Yi S, Chen J, et al. Does ensemble really work when facing the twitter semantic classification?[C]//2020 5th
International Conference on Computational Intelligence and Applications (ICCIA). IEEE, 2020: 39-43.

[6] de Ara√∫jo A F, Marcacini R M. RE-BERT: automatic extraction of software requirements from app reviews using BERT
language model[C]//Proceedings of the 36th Annual ACM Symposium on Applied Computing. 2021: 1321-1327.

[7] Das A, Zhang W E. ABSA-Bench: Towards the Unified Evaluation of Aspect-based Sentiment Analysis Research[C]
//Proceedings of The 18th Annual Workshop of the Australasian Language Technology Association. 2020: 65-71.

[8] Gui T, Wang X, Zhang Q, et al. Textflint: Unified multilingual robustness evaluation toolkit for natural language
processing[J]. arXiv preprint arXiv:2103.11441, 2021.

[9] Yang H, Zeng B. Enhancing Fine-grained Sentiment Classification Exploiting Local Context Embedding[J]. arXiv
preprint arXiv:2010.00767, 2020.

[10] Orbach M, Toledo-Ronen O, Spector A, et al. YASO: A New Benchmark for Targeted Sentiment Analysis[J]. arXiv
preprint arXiv:2012.14541, 2020.

[11] Hamborg F, Donnay K, Gipp B. Towards Target-dependent Sentiment Classification in News Articles[J]. arXiv preprint
arXiv:2105.09660, 2021.

[12] Mukherjee R, Shetty S, Chattopadhyay S, et al. Reproducibility, Replicability and Beyond: Assessing Production
Readiness of Aspect Based Sentiment Analysis in the Wild[J]. arXiv preprint arXiv:2101.09449, 2021.
